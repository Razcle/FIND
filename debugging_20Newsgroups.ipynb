{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging 20Newsgroups\n",
    "- **Task**: Classify \"Christianity\" vs \"Atheism\" documents from the 20 Newsgroups dataset.\n",
    "- **Problem**: The 20Newsgroups dataset is special because it contains a lot of artifacts – tokens (e.g., person names, punctuation marks) which are not relevant, but strongly cooccur with one of the classes. For evaluation, we therefore used the Religion dataset by [Ribeiro et al. (2016)](https://arxiv.org/pdf/1602.04938.pdf), containing \"Christianity\" and \"Atheism\" web pages, as a target dataset.\n",
    "- **Solution**: We use our framework to identify the features detecting irrelevant words (that do not capture the meaning of Christianity/Atheism and cannot generalize to the Religion dataset) and disable such features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Notebook setup\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Set random seed to create reproducable results\n",
    "the_seed = 1234\n",
    "np.random.seed(the_seed)\n",
    "random.seed(the_seed)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(the_seed)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GloVe word embeddings: Please replace the string in the second line with a path to your GloVe embeddings file which can be download [here](http://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING_PATH = f\"../../CNNAnalysis/data/glove/glove.6B.{EMBEDDING_DIM}d.txt\" # Path to your glove embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'preprocessed_data/'\n",
    "MAIN_DATASET = '20Newsgroups'\n",
    "SECOND_DATASET = 'Religion'\n",
    "THIRD_DATASET = None\n",
    "GENDER_BIAS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'trained_models/' # Path to save your trained models\n",
    "MODEL_ARCH = 'CNN'\n",
    "MAXLEN = 150\n",
    "FILTERS = [(10, 2), (10, 3), (10, 4)] # Ten filters of each window size [2,3,4]\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:52, 7652.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# 0. Load GloVe embeddings\n",
    "embedding_matrix, vocab_size, index2word, word2index = find.get_embedding_matrix(EMBEDDING_PATH, EMBEDDING_DIM, pad_initialisation = \"zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 863/863 [00:10<00:00, 86.19it/s] \n",
      "100%|██████████| 216/216 [00:02<00:00, 106.57it/s]\n",
      "100%|██████████| 717/717 [00:07<00:00, 95.33it/s] \n",
      "100%|██████████| 1819/1819 [01:21<00:00, 22.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load datasets and prepare inputs\n",
    "# 1.1 Main dataset\n",
    "data_1 = pickle.load(open(DATA_PATH + f'all_data_{MAIN_DATASET}.pickle', 'rb'))\n",
    "class_names = data_1['class_names']\n",
    "X_train_1, X_validate_1, X_test_1 = find.get_data_matrix(data_1['text_train'], word2index, MAXLEN), \\\n",
    "                                    find.get_data_matrix(data_1['text_validate'], word2index, MAXLEN), \\\n",
    "                                    find.get_data_matrix(data_1['text_test'], word2index, MAXLEN)\n",
    "y_test_1 = data_1['y_test']\n",
    "gender_test_1 = data_1['gender_test'] if GENDER_BIAS else None\n",
    "\n",
    "# 1.2 Second dataset\n",
    "if SECOND_DATASET is not None:\n",
    "    data_2 = pickle.load(open(DATA_PATH + f'all_data_{SECOND_DATASET}.pickle', 'rb'))\n",
    "    X_test_2, y_test_2 = find.get_data_matrix(data_2['text_test'], word2index, MAXLEN), data_2['y_test']\n",
    "    gender_test_2 = data_2['gender_test'] if GENDER_BIAS else None\n",
    "else:\n",
    "    X_test_2, y_test_2, gender_test_2 = None, None, None\n",
    "\n",
    "# 1.3 Third dataset\n",
    "if THIRD_DATASET is not None:\n",
    "    data_3 = pickle.load(open(DATA_PATH + f'all_data_{THIRD_DATASET}.pickle', 'rb'))\n",
    "    X_test_3, y_test_3 = find.get_data_matrix(data_3['text_test'], word2index, MAXLEN), data_3['y_test']\n",
    "    gender_test_3 = data_3['gender_test'] if GENDER_BIAS else None\n",
    "else:\n",
    "    X_test_3, y_test_3, gender_test_2  = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the result directory\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "result_folder = MAIN_DATASET + '_' + MODEL_ARCH + '_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + '/'\n",
    "result_path = MODEL_PATH + result_folder\n",
    "os.mkdir(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     120000600   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 149, 10)      6010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 148, 10)      9010        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 147, 10)      12010       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 10)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 10)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 10)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30)           0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "masked_dense_1 (MaskedDense)    (None, 2)            122         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 120,027,752\n",
      "Trainable params: 27,092\n",
      "Non-trainable params: 120,000,660\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 3. Create a model\n",
    "if MODEL_ARCH == 'CNN':\n",
    "    model = find.get_CNN_model(vocab_size, EMBEDDING_DIM, embedding_matrix, MAXLEN, class_names, FILTERS)\n",
    "else:\n",
    "    assert False, f\"Unsupported model architecture: {MODEL_ARCH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 863 samples, validate on 216 samples\n",
      "Epoch 1/300\n",
      " - 3s - loss: 0.7094 - acc: 0.5272 - val_loss: 0.5997 - val_acc: 0.6759\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59968, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 2/300\n",
      " - 3s - loss: 0.5400 - acc: 0.7474 - val_loss: 0.4885 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59968 to 0.48853, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 3/300\n",
      " - 3s - loss: 0.4448 - acc: 0.8320 - val_loss: 0.4590 - val_acc: 0.8102\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48853 to 0.45903, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 4/300\n",
      " - 3s - loss: 0.3883 - acc: 0.8598 - val_loss: 0.4081 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.45903 to 0.40808, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 5/300\n",
      " - 3s - loss: 0.3401 - acc: 0.8876 - val_loss: 0.3951 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.40808 to 0.39512, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 6/300\n",
      " - 3s - loss: 0.2974 - acc: 0.9154 - val_loss: 0.3736 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.39512 to 0.37356, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 7/300\n",
      " - 3s - loss: 0.2625 - acc: 0.9363 - val_loss: 0.3618 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.37356 to 0.36184, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 8/300\n",
      " - 4s - loss: 0.2327 - acc: 0.9479 - val_loss: 0.3454 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.36184 to 0.34536, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 9/300\n",
      " - 3s - loss: 0.2051 - acc: 0.9594 - val_loss: 0.3248 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.34536 to 0.32484, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 10/300\n",
      " - 3s - loss: 0.1822 - acc: 0.9757 - val_loss: 0.3171 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.32484 to 0.31714, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 11/300\n",
      " - 3s - loss: 0.1602 - acc: 0.9849 - val_loss: 0.2986 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.31714 to 0.29861, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 12/300\n",
      " - 3s - loss: 0.1431 - acc: 0.9884 - val_loss: 0.2980 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.29861 to 0.29798, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 13/300\n",
      " - 3s - loss: 0.1265 - acc: 0.9907 - val_loss: 0.2836 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.29798 to 0.28357, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 14/300\n",
      " - 3s - loss: 0.1131 - acc: 0.9965 - val_loss: 0.2764 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.28357 to 0.27642, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 15/300\n",
      " - 3s - loss: 0.1012 - acc: 0.9942 - val_loss: 0.2692 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.27642 to 0.26922, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 16/300\n",
      " - 3s - loss: 0.0903 - acc: 0.9977 - val_loss: 0.2598 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.26922 to 0.25984, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 17/300\n",
      " - 3s - loss: 0.0804 - acc: 0.9977 - val_loss: 0.2603 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.25984\n",
      "Epoch 18/300\n",
      " - 3s - loss: 0.0723 - acc: 0.9977 - val_loss: 0.2468 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.25984 to 0.24677, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 19/300\n",
      " - 3s - loss: 0.0651 - acc: 0.9977 - val_loss: 0.2464 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.24677 to 0.24636, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 20/300\n",
      " - 3s - loss: 0.0586 - acc: 0.9977 - val_loss: 0.2419 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.24636 to 0.24189, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 21/300\n",
      " - 3s - loss: 0.0534 - acc: 0.9988 - val_loss: 0.2374 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.24189 to 0.23736, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 22/300\n",
      " - 3s - loss: 0.0483 - acc: 1.0000 - val_loss: 0.2357 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.23736 to 0.23570, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 23/300\n",
      " - 3s - loss: 0.0439 - acc: 1.0000 - val_loss: 0.2327 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.23570 to 0.23275, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 24/300\n",
      " - 3s - loss: 0.0399 - acc: 1.0000 - val_loss: 0.2290 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.23275 to 0.22899, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 25/300\n",
      " - 3s - loss: 0.0366 - acc: 1.0000 - val_loss: 0.2265 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.22899 to 0.22653, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 26/300\n",
      " - 3s - loss: 0.0336 - acc: 1.0000 - val_loss: 0.2254 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.22653 to 0.22538, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 27/300\n",
      " - 3s - loss: 0.0310 - acc: 1.0000 - val_loss: 0.2226 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.22538 to 0.22263, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 28/300\n",
      " - 3s - loss: 0.0285 - acc: 1.0000 - val_loss: 0.2212 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.22263 to 0.22115, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 29/300\n",
      " - 3s - loss: 0.0266 - acc: 1.0000 - val_loss: 0.2224 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.22115\n",
      "Epoch 30/300\n",
      " - 4s - loss: 0.0246 - acc: 1.0000 - val_loss: 0.2135 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.22115 to 0.21347, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN.h5\n",
      "Epoch 31/300\n",
      " - 3s - loss: 0.0229 - acc: 1.0000 - val_loss: 0.2205 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.21347\n",
      "Epoch 32/300\n",
      " - 3s - loss: 0.0214 - acc: 1.0000 - val_loss: 0.2164 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.21347\n",
      "Epoch 33/300\n",
      " - 3s - loss: 0.0200 - acc: 1.0000 - val_loss: 0.2139 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.21347\n"
     ]
    }
   ],
   "source": [
    "# 4. Train the model\n",
    "history = find.model_train(model, result_path + f'trained_{MODEL_ARCH}.h5', X_train_1, data_1['y_train'], X_validate_1, data_1['y_validate'], BATCH_SIZE, epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate with the original test set:\n",
      "{'per_class': {0: {'all_positive': 298,\n",
      "                   'all_true': 319,\n",
      "                   'class_f1': 0.826580226904376,\n",
      "                   'class_name': 'alt.atheism',\n",
      "                   'class_precision': 0.8557046979865772,\n",
      "                   'class_recall': 0.799373040752351,\n",
      "                   'true_positive': 255},\n",
      "               1: {'all_positive': 419,\n",
      "                   'all_true': 398,\n",
      "                   'class_f1': 0.8690330477356182,\n",
      "                   'class_name': 'soc.religion.christian',\n",
      "                   'class_precision': 0.847255369928401,\n",
      "                   'class_recall': 0.8919597989949749,\n",
      "                   'true_positive': 355}},\n",
      " 'total': {'accuracy': 0.8507670850767085,\n",
      "           'macro_f1': 0.8485632695814168,\n",
      "           'macro_precision': 0.8514800339574891,\n",
      "           'macro_recall': 0.845666419873663,\n",
      "           'micro_f1': 0.8507670850767085,\n",
      "           'micro_precision': 0.8507670850767085,\n",
      "           'micro_recall': 0.8507670850767085}}\n",
      "FPR = 0.2006269592476489\n",
      "FNR = 0.10804020100502512\n",
      "\n",
      "Evaluate with the out-of-domain test set 1:\n",
      "{'per_class': {0: {'all_positive': 279,\n",
      "                   'all_true': 819,\n",
      "                   'class_f1': 0.4571948998178506,\n",
      "                   'class_name': 'alt.atheism',\n",
      "                   'class_precision': 0.899641577060932,\n",
      "                   'class_recall': 0.3064713064713065,\n",
      "                   'true_positive': 251},\n",
      "               1: {'all_positive': 1540,\n",
      "                   'all_true': 1000,\n",
      "                   'class_f1': 0.7653543307086613,\n",
      "                   'class_name': 'soc.religion.christian',\n",
      "                   'class_precision': 0.6311688311688312,\n",
      "                   'class_recall': 0.972,\n",
      "                   'true_positive': 972}},\n",
      " 'total': {'accuracy': 0.6723474436503574,\n",
      "           'macro_f1': 0.696653942652964,\n",
      "           'macro_precision': 0.7654052041148816,\n",
      "           'macro_recall': 0.6392356532356532,\n",
      "           'micro_f1': 0.6723474436503574,\n",
      "           'micro_precision': 0.6723474436503574,\n",
      "           'micro_recall': 0.6723474436503574}}\n",
      "FPR = 0.6935286935286935\n",
      "FNR = 0.028\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluate the model\n",
    "if not GENDER_BIAS:\n",
    "    find.evaluate_all(model, class_names, BATCH_SIZE, X_test_1, y_test_1, X_test_2, y_test_2, X_test_3, y_test_3, result_path = result_path, model_name = 'original')\n",
    "else:\n",
    "    find.evaluate_all_gender(model, class_names, BATCH_SIZE, X_test_1, y_test_1, gender_test_1, X_test_2, y_test_2, gender_test_2, result_path = result_path, model_name = 'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model understanding and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:00<00:00, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedded_text_input (InputLayer (None, 150, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 149, 10)      6010        embedded_text_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 148, 10)      9010        embedded_text_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 147, 10)      12010       embedded_text_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 10)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 10)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 10)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 30)           0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 27,030\n",
      "Trainable params: 0\n",
      "Non-trainable params: 27,030\n",
      "__________________________________________________________________________________________________\n",
      "Num batches: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 13.34it/s]\n",
      " 23%|██▎       | 7/30 [00:15<00:53,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: cannot summarize different ngrams ['taking christianity', '> biblical', '> biblical'], 1.4369050860404968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:07<00:00,  2.27s/it]\n",
      "100%|██████████| 30/30 [00:24<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# 6. Generate wordclouds\n",
    "settings = {\n",
    "    'model_arch': MODEL_ARCH,\n",
    "    'filters': FILTERS,\n",
    "    'maxlen': MAXLEN,\n",
    "    'result_path': result_path,\n",
    "    'index2word': index2word,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'batch_size': BATCH_SIZE\n",
    "}\n",
    "all_wordclouds = find.generate_wordclouds(model, X_train_1, settings, max_examples = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get input from a human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_feature_enabled = [True for i in range(find.num_all_filters(FILTERS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI components from ipywidgets\n",
    "import ipywidgets as wgt\n",
    "\n",
    "def update_screen(feature_idx):\n",
    "    show_action_panel(feature_idx)\n",
    "    wordcloud = all_wordclouds[feature_idx]\n",
    "    f, ax = plt.subplots()\n",
    "    plt.rcParams['figure.figsize'] = [14, 7]\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    W = model.layers[-1].get_weights()[0] # For the final layer\n",
    "    weight_plot = find.visualize_weights(W, feature_idx, class_names, show = False)\n",
    "    plt.show()\n",
    "\n",
    "def update_action(action):\n",
    "    global feature_radio_button, is_feature_enabled\n",
    "    feature_idx = feature_radio_button.value\n",
    "    if action == 'enabled':\n",
    "        print('enable')\n",
    "        is_feature_enabled[feature_idx] = True\n",
    "    elif action == 'disabled':\n",
    "        print('disable')\n",
    "        is_feature_enabled[feature_idx] = False\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "def show_action_panel(feature_idx):\n",
    "    global action_radio_button\n",
    "    action_radio_button.description = f'Current status of feature {feature_idx}:'\n",
    "    action_radio_button.value = 'enabled' if is_feature_enabled[feature_idx] else 'disabled'\n",
    "    \n",
    "feature_radio_button = wgt.RadioButtons(options=list(range(30)), value=0, description='Feature:', disabled=False)\n",
    "action_radio_button = wgt.RadioButtons(options=['enabled', 'disabled'],\n",
    "    value = 'enabled' if is_feature_enabled[feature_radio_button.value] else 'disabled',\n",
    "    description = f'Current status of feature {feature_radio_button.value}:',\n",
    "    style = {'description_width': 'initial'},\n",
    "    disabled = False\n",
    ")\n",
    "\n",
    "wgt.interactive_output(update_action, {'action':action_radio_button})\n",
    "out = wgt.interactive_output(update_screen, {'feature_idx':feature_radio_button})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61eca316a98484f8746d2d2125a4373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(RadioButtons(description='Feature:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. Get input from a human \n",
    "# Please investigate word clouds of these features and disable some irrelevant features using the radio-buttons under the bar plot. \n",
    "# Once you are happy, please then proceed to the next cell.\n",
    "display(wgt.HBox([feature_radio_button, wgt.VBox([out, action_radio_button])]))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 30 features \n",
      "Enabled: 19 features \n",
      "Disabled: 11 features\n",
      "Disabled features: [1, 2, 3, 9, 12, 16, 20, 24, 25, 26, 28]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total: {len(is_feature_enabled)} features \\nEnabled: {sum(is_feature_enabled)} features \\nDisabled: {len(is_feature_enabled)-sum(is_feature_enabled)} features\")\n",
    "print(f\"Disabled features: {[i for i,s in enumerate(is_feature_enabled) if not s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and fine-tuning an improved classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 150, 300)     120000600   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 149, 10)      6010        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 148, 10)      9010        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 147, 10)      12010       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 10)           0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 10)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 10)           0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 30)           0           global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "masked_dense_2 (MaskedDense)    (None, 2)            122         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 120,027,752\n",
      "Trainable params: 62\n",
      "Non-trainable params: 120,027,690\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 8. Create an improved model\n",
    "# 8.1 Copy the existing CNN features\n",
    "model_improved = find.get_CNN_model(vocab_size, EMBEDDING_DIM, embedding_matrix, MAXLEN, class_names, \n",
    "                                    FILTERS, trainable_filters = False)\n",
    "model_improved.set_weights(model.get_weights()) \n",
    "\n",
    "# 8.2 Apply human decisions to disable irrelevant features\n",
    "for idx, enable in enumerate(is_feature_enabled):\n",
    "    if not enable:\n",
    "        model_improved.layers[-1].disable_mask(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 863 samples, validate on 216 samples\n",
      "Epoch 1/300\n",
      " - 2s - loss: 0.7720 - acc: 0.5829 - val_loss: 1.0870 - val_acc: 0.5694\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.08697, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 2/300\n",
      " - 2s - loss: 0.6408 - acc: 0.6188 - val_loss: 0.9389 - val_acc: 0.6019\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.08697 to 0.93887, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 3/300\n",
      " - 2s - loss: 0.5213 - acc: 0.6628 - val_loss: 0.8066 - val_acc: 0.6296\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.93887 to 0.80661, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 4/300\n",
      " - 2s - loss: 0.4220 - acc: 0.7323 - val_loss: 0.6912 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.80661 to 0.69116, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 5/300\n",
      " - 2s - loss: 0.3405 - acc: 0.8042 - val_loss: 0.5952 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.69116 to 0.59522, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 6/300\n",
      " - 2s - loss: 0.2791 - acc: 0.8633 - val_loss: 0.5178 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.59522 to 0.51777, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 7/300\n",
      " - 2s - loss: 0.2336 - acc: 0.9131 - val_loss: 0.4580 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51777 to 0.45802, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 8/300\n",
      " - 2s - loss: 0.2012 - acc: 0.9467 - val_loss: 0.4143 - val_acc: 0.8241\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.45802 to 0.41433, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 9/300\n",
      " - 2s - loss: 0.1799 - acc: 0.9618 - val_loss: 0.3834 - val_acc: 0.8380\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.41433 to 0.38336, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 10/300\n",
      " - 2s - loss: 0.1664 - acc: 0.9722 - val_loss: 0.3625 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.38336 to 0.36251, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 11/300\n",
      " - 2s - loss: 0.1580 - acc: 0.9768 - val_loss: 0.3493 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36251 to 0.34928, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 12/300\n",
      " - 2s - loss: 0.1536 - acc: 0.9849 - val_loss: 0.3407 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.34928 to 0.34075, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 13/300\n",
      " - 2s - loss: 0.1512 - acc: 0.9849 - val_loss: 0.3352 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.34075 to 0.33516, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 14/300\n",
      " - 2s - loss: 0.1497 - acc: 0.9861 - val_loss: 0.3317 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.33516 to 0.33170, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 15/300\n",
      " - 2s - loss: 0.1487 - acc: 0.9873 - val_loss: 0.3294 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.33170 to 0.32940, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 16/300\n",
      " - 2s - loss: 0.1478 - acc: 0.9873 - val_loss: 0.3278 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.32940 to 0.32781, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 17/300\n",
      " - 2s - loss: 0.1469 - acc: 0.9873 - val_loss: 0.3269 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.32781 to 0.32692, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 18/300\n",
      " - 2s - loss: 0.1460 - acc: 0.9873 - val_loss: 0.3263 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.32692 to 0.32633, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 19/300\n",
      " - 2s - loss: 0.1451 - acc: 0.9873 - val_loss: 0.3260 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.32633 to 0.32597, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 20/300\n",
      " - 2s - loss: 0.1442 - acc: 0.9873 - val_loss: 0.3256 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.32597 to 0.32556, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 21/300\n",
      " - 2s - loss: 0.1432 - acc: 0.9873 - val_loss: 0.3249 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.32556 to 0.32492, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 22/300\n",
      " - 2s - loss: 0.1423 - acc: 0.9873 - val_loss: 0.3244 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.32492 to 0.32437, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 23/300\n",
      " - 2s - loss: 0.1413 - acc: 0.9873 - val_loss: 0.3238 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.32437 to 0.32379, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 24/300\n",
      " - 2s - loss: 0.1404 - acc: 0.9884 - val_loss: 0.3229 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.32379 to 0.32288, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 25/300\n",
      " - 2s - loss: 0.1394 - acc: 0.9884 - val_loss: 0.3224 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.32288 to 0.32242, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 26/300\n",
      " - 2s - loss: 0.1385 - acc: 0.9884 - val_loss: 0.3221 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.32242 to 0.32207, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 27/300\n",
      " - 2s - loss: 0.1375 - acc: 0.9884 - val_loss: 0.3216 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.32207 to 0.32159, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 28/300\n",
      " - 2s - loss: 0.1366 - acc: 0.9884 - val_loss: 0.3208 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.32159 to 0.32085, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 29/300\n",
      " - 2s - loss: 0.1356 - acc: 0.9884 - val_loss: 0.3202 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.32085 to 0.32015, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 30/300\n",
      " - 2s - loss: 0.1346 - acc: 0.9884 - val_loss: 0.3196 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.32015 to 0.31961, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 31/300\n",
      " - 2s - loss: 0.1337 - acc: 0.9884 - val_loss: 0.3191 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.31961 to 0.31909, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 32/300\n",
      " - 2s - loss: 0.1327 - acc: 0.9884 - val_loss: 0.3181 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.31909 to 0.31812, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 33/300\n",
      " - 2s - loss: 0.1318 - acc: 0.9884 - val_loss: 0.3171 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.31812 to 0.31707, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 34/300\n",
      " - 2s - loss: 0.1308 - acc: 0.9884 - val_loss: 0.3165 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.31707 to 0.31654, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 35/300\n",
      " - 2s - loss: 0.1299 - acc: 0.9896 - val_loss: 0.3158 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.31654 to 0.31581, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 36/300\n",
      " - 2s - loss: 0.1289 - acc: 0.9884 - val_loss: 0.3154 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.31581 to 0.31544, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/300\n",
      " - 2s - loss: 0.1280 - acc: 0.9884 - val_loss: 0.3151 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.31544 to 0.31510, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 38/300\n",
      " - 2s - loss: 0.1271 - acc: 0.9884 - val_loss: 0.3141 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.31510 to 0.31409, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 39/300\n",
      " - 2s - loss: 0.1261 - acc: 0.9896 - val_loss: 0.3140 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.31409 to 0.31402, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 40/300\n",
      " - 2s - loss: 0.1252 - acc: 0.9884 - val_loss: 0.3131 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.31402 to 0.31307, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 41/300\n",
      " - 2s - loss: 0.1242 - acc: 0.9896 - val_loss: 0.3125 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.31307 to 0.31247, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 42/300\n",
      " - 2s - loss: 0.1233 - acc: 0.9896 - val_loss: 0.3117 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.31247 to 0.31170, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 43/300\n",
      " - 2s - loss: 0.1224 - acc: 0.9896 - val_loss: 0.3113 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.31170 to 0.31130, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 44/300\n",
      " - 2s - loss: 0.1215 - acc: 0.9896 - val_loss: 0.3106 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.31130 to 0.31058, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 45/300\n",
      " - 2s - loss: 0.1206 - acc: 0.9907 - val_loss: 0.3101 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.31058 to 0.31013, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 46/300\n",
      " - 2s - loss: 0.1197 - acc: 0.9907 - val_loss: 0.3096 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.31013 to 0.30964, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 47/300\n",
      " - 2s - loss: 0.1189 - acc: 0.9907 - val_loss: 0.3092 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.30964 to 0.30921, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 48/300\n",
      " - 2s - loss: 0.1180 - acc: 0.9907 - val_loss: 0.3083 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.30921 to 0.30832, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 49/300\n",
      " - 2s - loss: 0.1171 - acc: 0.9907 - val_loss: 0.3078 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.30832 to 0.30776, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 50/300\n",
      " - 2s - loss: 0.1163 - acc: 0.9919 - val_loss: 0.3071 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.30776 to 0.30707, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 51/300\n",
      " - 2s - loss: 0.1154 - acc: 0.9919 - val_loss: 0.3067 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.30707 to 0.30670, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 52/300\n",
      " - 2s - loss: 0.1146 - acc: 0.9919 - val_loss: 0.3061 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.30670 to 0.30612, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 53/300\n",
      " - 2s - loss: 0.1137 - acc: 0.9919 - val_loss: 0.3057 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.30612 to 0.30566, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 54/300\n",
      " - 2s - loss: 0.1129 - acc: 0.9919 - val_loss: 0.3055 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.30566 to 0.30549, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 55/300\n",
      " - 2s - loss: 0.1121 - acc: 0.9919 - val_loss: 0.3048 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.30549 to 0.30485, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 56/300\n",
      " - 2s - loss: 0.1113 - acc: 0.9919 - val_loss: 0.3041 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.30485 to 0.30413, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 57/300\n",
      " - 2s - loss: 0.1105 - acc: 0.9919 - val_loss: 0.3037 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.30413 to 0.30369, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 58/300\n",
      " - 2s - loss: 0.1097 - acc: 0.9919 - val_loss: 0.3029 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.30369 to 0.30292, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 59/300\n",
      " - 2s - loss: 0.1089 - acc: 0.9919 - val_loss: 0.3025 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.30292 to 0.30247, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 60/300\n",
      " - 2s - loss: 0.1081 - acc: 0.9919 - val_loss: 0.3018 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.30247 to 0.30181, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 61/300\n",
      " - 2s - loss: 0.1073 - acc: 0.9919 - val_loss: 0.3017 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.30181 to 0.30171, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 62/300\n",
      " - 2s - loss: 0.1066 - acc: 0.9919 - val_loss: 0.3014 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.30171 to 0.30139, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 63/300\n",
      " - 2s - loss: 0.1058 - acc: 0.9919 - val_loss: 0.3012 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.30139 to 0.30116, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 64/300\n",
      " - 2s - loss: 0.1051 - acc: 0.9919 - val_loss: 0.3008 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.30116 to 0.30081, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 65/300\n",
      " - 2s - loss: 0.1043 - acc: 0.9919 - val_loss: 0.3007 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.30081 to 0.30066, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 66/300\n",
      " - 2s - loss: 0.1036 - acc: 0.9919 - val_loss: 0.3006 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.30066 to 0.30059, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 67/300\n",
      " - 2s - loss: 0.1029 - acc: 0.9919 - val_loss: 0.2994 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.30059 to 0.29938, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 68/300\n",
      " - 2s - loss: 0.1022 - acc: 0.9919 - val_loss: 0.2982 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.29938 to 0.29823, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 69/300\n",
      " - 2s - loss: 0.1014 - acc: 0.9919 - val_loss: 0.2976 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.29823 to 0.29760, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 70/300\n",
      " - 2s - loss: 0.1007 - acc: 0.9919 - val_loss: 0.2975 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.29760 to 0.29748, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 71/300\n",
      " - 2s - loss: 0.1000 - acc: 0.9919 - val_loss: 0.2974 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.29748 to 0.29742, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 72/300\n",
      " - 2s - loss: 0.0993 - acc: 0.9919 - val_loss: 0.2970 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.29742 to 0.29704, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/300\n",
      " - 2s - loss: 0.0987 - acc: 0.9919 - val_loss: 0.2968 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.29704 to 0.29682, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 74/300\n",
      " - 2s - loss: 0.0980 - acc: 0.9919 - val_loss: 0.2962 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.29682 to 0.29621, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 75/300\n",
      " - 2s - loss: 0.0973 - acc: 0.9919 - val_loss: 0.2959 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.29621 to 0.29595, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 76/300\n",
      " - 2s - loss: 0.0967 - acc: 0.9919 - val_loss: 0.2959 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.29595 to 0.29589, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 77/300\n",
      " - 2s - loss: 0.0960 - acc: 0.9919 - val_loss: 0.2952 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.29589 to 0.29522, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 78/300\n",
      " - 2s - loss: 0.0954 - acc: 0.9919 - val_loss: 0.2951 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.29522 to 0.29506, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 79/300\n",
      " - 2s - loss: 0.0947 - acc: 0.9919 - val_loss: 0.2946 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.29506 to 0.29460, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 80/300\n",
      " - 2s - loss: 0.0941 - acc: 0.9919 - val_loss: 0.2938 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.29460 to 0.29384, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 81/300\n",
      " - 2s - loss: 0.0935 - acc: 0.9930 - val_loss: 0.2935 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.29384 to 0.29348, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 82/300\n",
      " - 2s - loss: 0.0929 - acc: 0.9930 - val_loss: 0.2924 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.29348 to 0.29245, saving model to trained_models/20Newsgroups_CNN_20201010014944/trained_CNN_improved.h5\n",
      "Epoch 83/300\n",
      " - 2s - loss: 0.0923 - acc: 0.9930 - val_loss: 0.2927 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.29245\n",
      "Epoch 84/300\n",
      " - 2s - loss: 0.0917 - acc: 0.9930 - val_loss: 0.2928 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.29245\n",
      "Epoch 85/300\n",
      " - 2s - loss: 0.0911 - acc: 0.9930 - val_loss: 0.2925 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.29245\n"
     ]
    }
   ],
   "source": [
    "# 9. Fine-tuning the improved model\n",
    "history = find.model_train(model_improved, result_path + f'trained_{MODEL_ARCH}_improved.h5', X_train_1, data_1['y_train'], X_validate_1, data_1['y_validate'], BATCH_SIZE, epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate with the original test set:\n",
      "{'per_class': {0: {'all_positive': 286,\n",
      "                   'all_true': 319,\n",
      "                   'class_f1': 0.7867768595041322,\n",
      "                   'class_name': 'alt.atheism',\n",
      "                   'class_precision': 0.8321678321678322,\n",
      "                   'class_recall': 0.7460815047021944,\n",
      "                   'true_positive': 238},\n",
      "               1: {'all_positive': 431,\n",
      "                   'all_true': 398,\n",
      "                   'class_f1': 0.8443908323281063,\n",
      "                   'class_name': 'soc.religion.christian',\n",
      "                   'class_precision': 0.8120649651972158,\n",
      "                   'class_recall': 0.8793969849246231,\n",
      "                   'true_positive': 350}},\n",
      " 'total': {'accuracy': 0.8200836820083682,\n",
      "           'macro_f1': 0.8174009291550225,\n",
      "           'macro_precision': 0.822116398682524,\n",
      "           'macro_recall': 0.8127392448134088,\n",
      "           'micro_f1': 0.8200836820083681,\n",
      "           'micro_precision': 0.8200836820083682,\n",
      "           'micro_recall': 0.8200836820083682}}\n",
      "FPR = 0.25391849529780564\n",
      "FNR = 0.12060301507537688\n",
      "\n",
      "Evaluate with the out-of-domain test set 1:\n",
      "{'per_class': {0: {'all_positive': 617,\n",
      "                   'all_true': 819,\n",
      "                   'class_f1': 0.7715877437325905,\n",
      "                   'class_name': 'alt.atheism',\n",
      "                   'class_precision': 0.8978930307941653,\n",
      "                   'class_recall': 0.6764346764346765,\n",
      "                   'true_positive': 554},\n",
      "               1: {'all_positive': 1202,\n",
      "                   'all_true': 1000,\n",
      "                   'class_f1': 0.8510445049954587,\n",
      "                   'class_name': 'soc.religion.christian',\n",
      "                   'class_precision': 0.7795341098169717,\n",
      "                   'class_recall': 0.937,\n",
      "                   'true_positive': 937}},\n",
      " 'total': {'accuracy': 0.8196811434854315,\n",
      "           'macro_f1': 0.822404362843837,\n",
      "           'macro_precision': 0.8387135703055685,\n",
      "           'macro_recall': 0.8067173382173383,\n",
      "           'micro_f1': 0.8196811434854315,\n",
      "           'micro_precision': 0.8196811434854315,\n",
      "           'micro_recall': 0.8196811434854315}}\n",
      "FPR = 0.3235653235653236\n",
      "FNR = 0.063\n"
     ]
    }
   ],
   "source": [
    "# 10. Evaluate the improved model\n",
    "if not GENDER_BIAS:\n",
    "    find.evaluate_all(model_improved, class_names, BATCH_SIZE, X_test_1, y_test_1, X_test_2, y_test_2, X_test_3, y_test_3, result_path = result_path, model_name = 'debugged')\n",
    "else:\n",
    "    find.evaluate_all_gender(model_improved, class_names, BATCH_SIZE, X_test_1, y_test_1, gender_test_1, X_test_2, y_test_2, gender_test_2, result_path = result_path, model_name = 'debugged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
